{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db064ab2",
   "metadata": {},
   "source": [
    "<font size = 5>**LAB 14 PART 1**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e816a6",
   "metadata": {},
   "source": [
    "Implement the forward propagation for a two hidden layer network for m-samples, n-features as we discussed in class. Initialize the weights randomly. Use the data from the previous labs like logistic regression. You can choose the number of neurons in the hidden layer and use sigmoid activation function.Report the evaluation metrics for the network.  Also use other non-linear activation functions like ReLU and Tanh. Report the loss using both MSE and Cross Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb64b419",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79fb25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8816178",
   "metadata": {},
   "source": [
    "### Loss functions : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d9c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MSEloss(target,predicted):\n",
    "    m = target.shape[1]\n",
    "    return np.mean((target-predicted)**2)\n",
    "\n",
    "def CrossEntropyLoss(target,predicted):\n",
    "    m = target.shape[1]\n",
    "    losses = -target*np.log(predicted) - (1-target)*np.log(1-predicted)\n",
    "    return np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62edcd",
   "metadata": {},
   "source": [
    "### Activation Functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c246e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = np.array([[-1,2],[-4,5]])\\napp = np.vectorize(ReLU)\\nprint(app(x))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "\n",
    "'''\n",
    "x = np.array([-1,2,-4,5])\n",
    "tanh(x)\n",
    "'''\n",
    "\n",
    "def ReLU(x):\n",
    "    if(x < 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "#To use function on all elements of a numpy array\n",
    "ReLU_apply = np.vectorize(ReLU)\n",
    "'''\n",
    "x = np.array([-1,2,-4,5])\n",
    "app = np.vectorize(ReLU)\n",
    "app(x)\n",
    "'''\n",
    "'''\n",
    "x = np.array([[-1,2],[-4,5]])\n",
    "app = np.vectorize(ReLU)\n",
    "print(app(x))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af46c4c",
   "metadata": {},
   "source": [
    "### Neural Network class\n",
    "##### Note that I have done the forward propogation for 2 hidden layers\n",
    "##### neural network class for variable N - hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22edec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network_FP:\n",
    "    def __init__(self,act_func):\n",
    "        self.activation_func = act_func\n",
    "    def forward_prop(self,x):\n",
    "        #We are given x which is nxm size . features x samples\n",
    "        n = x.shape[0]\n",
    "        m = x.shape[1]\n",
    "        \n",
    "        n1 = 3 #number of neurons in first hidden layer\n",
    "        n2 = 3 #number of neurons in second hidden layer\n",
    "        \n",
    "        #np.random.rand(a,b) returns a numpy array of shape (a,b) and numbers in interval 0 to 1\n",
    "        #I have multiplied by 4 and subtracted 2 from each element => so that the elements turn out to come in the interval -2 to 2 \n",
    "        \n",
    "        w1 = np.random.rand(n1,n)*4-2\n",
    "        b1 = np.random.rand(n1,m)*4-2\n",
    "        w2 = np.random.rand(n2,n1)*4-2\n",
    "        b2 = np.random.rand(n2,m)*4-2\n",
    "        w3 = np.random.rand(1,n2)*4-2\n",
    "        b3 = np.random.rand(1,m)*4-2\n",
    "        \n",
    "        z1 = w1@x + b1\n",
    "        a1 = self.activation_func(z1)\n",
    "        z2 = w2@a1 + b2\n",
    "        a2 = self.activation_func(z2)\n",
    "        z3 = w3@a2 + b3\n",
    "        a3 = sigmoid(z3)\n",
    "    \n",
    "        return a3\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f887823",
   "metadata": {},
   "source": [
    "### Read data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc7169fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"logistic_regression_ls.csv\")\n",
    "x = np.array(df[[\"x1\",'x2']]).T\n",
    "target = np.array(df[\"label\"]).reshape((1,x.shape[1]))\n",
    "\n",
    "#Have the features data and targets now ! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c779e",
   "metadata": {},
   "source": [
    "### Sigmoid Activation function neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afefd2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss :  0.32662024353671454\n",
      "Cross Entropy Loss : 0.908938155496284\n"
     ]
    }
   ],
   "source": [
    "model = neural_network_FP(sigmoid)\n",
    "predicted = model.forward_prop(x)\n",
    "\n",
    "lossMSE = MSEloss(target,predicted)\n",
    "lossCross = CrossEntropyLoss(target,predicted)\n",
    "\n",
    "print(\"MSE loss : \",lossMSE)\n",
    "print(\"Cross Entropy Loss :\",lossCross)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7978e6f2",
   "metadata": {},
   "source": [
    "### Evaluation Metrics Applied on predictions\n",
    "#### Note that the predictions are similar because the initialised weights are random . So avaerages out to similar predictions.\n",
    "(Actually random between 0 and 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b3751ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "158,165\n",
      "92,85\n",
      "Precision: 0.4891640866873065\n",
      "Recall: 0.632\n",
      "F1_score: 0.5514834205933683\n",
      "Accuracy: 0.486\n"
     ]
    }
   ],
   "source": [
    "#Evaluation Metrics\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "\n",
    "#print(predicted)\n",
    "\n",
    "for i in range(500):\n",
    "    if(target[0][i] == 1):\n",
    "        if(predicted[0][i] < 0.5):\n",
    "            FN = FN + 1\n",
    "        else:\n",
    "            TP = TP + 1\n",
    "    else:\n",
    "        if(predicted[0][i] < 0.5):\n",
    "            TN = TN + 1\n",
    "        else:\n",
    "            FP = FP + 1\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{TP},{FP}\\n{FN},{TN}\")\n",
    "\n",
    "Precision = TP/(TP + FP)\n",
    "Recall = TP/(TP + FN)\n",
    "F1_score = 2*Precision*Recall/(Precision+Recall)\n",
    "Accuracy = (TN + TP)/(TN + TP + FN + FP)\n",
    "\n",
    "print(f'''Precision: {Precision}\n",
    "Recall: {Recall}\n",
    "F1_score: {F1_score}\n",
    "Accuracy: {Accuracy}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de1cc0a",
   "metadata": {},
   "source": [
    "### Trying out other activation functions\n",
    "#### tanh , tanh function defined by me above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c1bfd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss :  0.3211976795232872\n",
      "Cross Entropy Loss : 0.9564769953532664\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = neural_network_FP(tanh)\n",
    "predicted = model.forward_prop(x)\n",
    "\n",
    "lossMSE = MSEloss(target,predicted)\n",
    "lossCross = CrossEntropyLoss(target,predicted)\n",
    "\n",
    "print(\"MSE loss : \",lossMSE)\n",
    "print(\"Cross Entropy Loss :\",lossCross)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f54d92a",
   "metadata": {},
   "source": [
    "#### ReLU_apply is vectorized function of ReLU so that it can be applied on a numpy array. I created it above in Activation function section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c0460bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss :  0.5332173401779241\n",
      "Cross Entropy Loss : 4.25818300713555\n"
     ]
    }
   ],
   "source": [
    "model = neural_network_FP(ReLU_apply)\n",
    "predicted = model.forward_prop(x)\n",
    "\n",
    "lossMSE = MSEloss(target,predicted)\n",
    "lossCross = CrossEntropyLoss(target,predicted)\n",
    "\n",
    "print(\"MSE loss : \",lossMSE)\n",
    "print(\"Cross Entropy Loss :\",lossCross)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9907de2b",
   "metadata": {},
   "source": [
    "### Note that due to ReLU in between , we can have abnormally close value of output to 1. So there may be error in cross entropy calc. sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9229ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6398c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
